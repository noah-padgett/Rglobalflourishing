% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/gfs_svyglm.R
\name{gfs_svyglm}
\alias{gfs_svyglm}
\title{GFS Wrapper for Regression
Runs relatively simple regression analysis}
\usage{
gfs_svyglm(
  formula,
  svy.design,
  family = gaussian(),
  robust.huberM = FALSE,
  robust.tune = 1,
  ...
)
}
\arguments{
\item{formula}{a model (linear) as used in glm}

\item{svy.design}{a data.frame or survey.design object}

\item{family}{the usual family argument as used in glms see below for more information}

\item{robust.huberM}{a logical defining whether to use the robsurvey pacakge to estimate the linear regression model instead of the usual svyglm(.) function (default: FALSE)}

\item{robust.tune}{(default 1) tuning parameter for robsurvey package}

\item{...}{additional arguments as needed}
}
\value{
a list of containing resulting fitted object (fit), residuals, a tidied version, and
a vector containing the included predictor variables.
}
\description{
#' When robust.huberM is TRUE, for (approximately) continuous outcomes, you have the option of
alternatively using a "robust m-estimator" with Huber style robustness weights in addition to
the complex sampling design adjustments. It's unknown whether this makes a meaningful difference,
but preliminary testing suggests small differences in point estimates but sometimes dramatic
changes to standard errors for reasons that are unclear to me. Could be due to a strange
interaction of robustness weights, attrition weights, and post-stratified sampling weights.
}
\examples{
{
  # TO-DO
}
}
